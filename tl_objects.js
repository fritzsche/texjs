const fs = require('fs/promises')
const path = require('path')
// const zlib = require('zlib'); // Removed zlib as compression is no longer needed
const crypto = require('crypto')

// --- Configuration ---
const SOURCE_DIR = './texlive'
const TARGET_DIR = './objects' // Target directory for custom objects
// ---------------------

// ======================================================================
// 1. Constant Definition for Size Limit
// Define the maximum allowed size for any generated object file/blob.
// Value is 30 MB, converted into bytes (30 * 1024 * 1024).
// ======================================================================
const MAX_BLOB_SIZE_MB = 30;
const MAX_BLOB_SIZE_BYTES = MAX_BLOB_SIZE_MB * 1024 * 1024;


/**
 * Writes a buffer as an uncompressed file to the target directory.
 * The file name is the SHA-1 hash of the content.
 * @param {Buffer} content - The content to be stored.
 * @returns {Promise<string|null>} - The 40-character SHA-1 hash of the content, or null if skipped.
 */
async function writeObject(content) {
    const contentSize = content.length;

    // --- Core Size Check (Modified Logic) ---
    if (contentSize > MAX_BLOB_SIZE_BYTES) {
        const sizeMB = (contentSize / (1024 * 1024)).toFixed(2);
        const limitMB = MAX_BLOB_SIZE_MB;
        // Log a warning and return null to signal the skip
        console.warn(`WARNING: Object size (${sizeMB} MB) exceeds the maximum allowed limit of ${limitMB} MB. Object write skipped.`);
        return null;
    }

    // 1. Calculate SHA-1 Hash (Hash is based on the content)
    const hash = crypto.createHash('sha1').update(content).digest('hex')

    // 2. Define the flat file path (<hash> - NO .gz extension)
    const filePath = path.join(TARGET_DIR, `${hash}`) // Removed '.gz'

    // 3. Write the content directly (no compression)
    // We write the original 'content' buffer.
    await fs.writeFile(filePath, content)

    return hash
}

/**
 * Generates an ES6 module file (root.js) exporting the root hash.
 * This is stored in the objects directory alongside the generated objects.
 * @param {string} rootHash - The SHA-1 hash of the root tree object.
 * @returns {Promise<void>}
 */
async function generateRootModule(rootHash) {
    // Content of the ES6 module
    const moduleContent = `// This file is auto-generated by the object generator script.
// It exports the SHA-1 hash of the root tree object for easy access.
export const ROOT_HASH = '${rootHash}';
`
    // Define the full path for the output file
    const filePath = path.join(TARGET_DIR, 'root.js')

    // Write the content to the file system
    await fs.writeFile(filePath, moduleContent, 'utf8')

    console.log(`\nSuccessfully generated ES6 module: ${filePath}`)
}

/**
 * Processes a single file as a "Blob."
 * Content is the raw file data (no Git header), stored uncompressed as <hash>.
 * @param {string} filePath - The full path to the file.
 * @returns {Promise<object|null>} - Blob metadata, or null if the blob was skipped.
 */
async function processBlob(filePath) {
    const data = await fs.readFile(filePath)
    const stat = await fs.stat(filePath)

    // 1. Write Blob data (data is the pure file content)
    const hash = await writeObject(data)

    // NEW CHECK: If writeObject returned null, return null to signal processTree to skip this entry
    if (!hash) {
        return null;
    }

    // ADDED: Output message for each blob generated
    console.log(`Generated Blob: ${hash} from ${path.basename(filePath)}`)

    // 2. Return metadata for the Tree entry
    return {
        hash,
        mode: stat.mode.toString(8), // Unix permissions (Octal)
        type: 'blob',
        size: stat.size,           // File size in bytes
        name: path.basename(filePath)
    }
}

/**
 * Recursively processes a directory as a "Tree."
 * Content is a JSON array of metadata, stored uncompressed as <hash>.
 * @param {string} dirPath - The full path to the directory.
 * @returns {Promise<string|null>} - The SHA-1 hash of the generated Tree JSON, or null if the tree itself was skipped.
 */
async function processTree(dirPath) {
    const entries = await fs.readdir(dirPath, { withFileTypes: true })
    let treeEntries = []

    for (const entry of entries) {
        // Ignore .git and hidden files/the target directory itself
        if (entry.name === '.git' || entry.name.startsWith('.') || entry.name === TARGET_DIR.replace('./', '')) continue

        const fullPath = path.join(dirPath, entry.name)
        let objectInfo = null // Initialize to null

        if (entry.isDirectory()) {
            // Recursive call for subdirectory
            const treeHash = await processTree(fullPath)
            
            // NEW CHECK: Skip the subdirectory if its resulting tree object was too large
            if (!treeHash) {
                continue;
            }

            const stat = await fs.stat(fullPath)
            objectInfo = {
                hash: treeHash,
                mode: stat.mode.toString(8),
                type: 'tree',
                name: entry.name
            }
        } else if (entry.isFile()) {
            // Process file as Blob. Returns null if oversized.
            objectInfo = await processBlob(fullPath)
        } else {
            // Ignore other types
            continue
        }

        // MODIFIED: Only push to treeEntries if objectInfo is not null (i.e., not a skipped large blob or tree)
        if (objectInfo) {
            treeEntries.push(objectInfo)
        }
    }

    // Sort by name for consistency
    treeEntries.sort((a, b) => a.name.localeCompare(b.name))

    // 1. Create JSON content
    const jsonContent = JSON.stringify(treeEntries, null, 2)
    const contentBuffer = Buffer.from(jsonContent, 'utf8')

    // 2. Write Tree JSON and retrieve hash (as <hash>)
    const treeHash = await writeObject(contentBuffer)
    
    // NEW CHECK: If the Tree object itself was too large, return null to the caller.
    if (!treeHash) {
        return null;
    }

    console.log(`Generated Tree: ${treeHash} for directory ${dirPath}`)
    return treeHash
}

/**
 * Main function to start the process.
 */
async function main() {
    console.log(`Starting Git object generation (browser-optimized, flat structure) from: ${SOURCE_DIR}`)
    console.log(`Target directory: ${TARGET_DIR}`)

    try {
        await fs.access(SOURCE_DIR)
        // Ensure the single target directory exists (no subfolders needed)
        await fs.mkdir(TARGET_DIR, { recursive: true })

        const rootTreeHash = await processTree(SOURCE_DIR)
        
        // NEW CHECK: Only generate root module and success message if a hash was actually created
        if (rootTreeHash) {
            await generateRootModule(rootTreeHash)
            console.log('\n--- PROCESSING COMPLETE ---')
            console.log(`The SHA-1 hash of the top-level (Root) Tree object is: ${rootTreeHash}`)
            console.log(`All objects are stored directly in: ${TARGET_DIR}`)
        } else {
            console.error('\n--- PROCESSING ABORTED ---')
            console.error('The root tree object could not be generated, likely due to the resulting JSON being too large for the configured limit. No root module was created.')
        }

    } catch (error) {
        console.error('\nERROR DURING PROCESSING:', error.message)
        // Keep the general instruction for file system issues
        console.error('Ensure that the source directory (./texlive) exists and contains data.')
    }
}

main()
